{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "In this lab, we will learn how to acquire data from Hugging Face datasets and how to bring it into the right format for our models.Fill out the missing pieces in the source source to get everything working (indicated by `#FIXME`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with the wikiqa dataset. It is a question answering dataset, where the task is to predict whether a given question can be answered by a given sentence from a Wikipedia article. The dataset is available in the [Hugging Face datasets library](https://huggingface.co/datasets/wiki_qa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiqa = load_dataset(\"wiki_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert a dataset to a pandas dataframe with the document title and text columns\n",
    "def to_dataframe(dataset):\n",
    "  return dataset.to_pandas()\n",
    "\n",
    "# Concatenate the train, test, and validation datasets using the function\n",
    "wikiqa_df = pd.concat([to_dataframe(wikiqa[\"train\"]), to_dataframe(wikiqa[\"test\"]), to_dataframe(wikiqa[\"validation\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine document_title, question, and answer into a single column and add a \\n as separator\n",
    "wikiqa_df[\"text\"] = wikiqa_df[\"document_title\"] + \"\\n\" + wikiqa_df[\"question\"] + \"\\n\" + wikiqa_df[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(wikiqa_df['document_title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiqa_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.environ.get(\"AOAI_ENDPOINT\")\n",
    "openai.api_key = os.environ.get(\"AOAI_KEY\")\n",
    "openai.api_version = \"2022-12-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Embedding.create(engine=\"text-embedding-ada-002\",\n",
    "                                    prompt=wikiqa_df['text'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def getdata(url):\n",
    "    r = requests.get(url)\n",
    "    return r.content\n",
    "\n",
    "# create empty dict\n",
    "dict_href_links = {}\n",
    "\n",
    "def get_links(website_link):\n",
    "    html_data = getdata(website_link)\n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    list_links = []\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        \n",
    "        # Append to list if new link contains original link\n",
    "        if str(link[\"href\"]).startswith((str(website_link))):\n",
    "            list_links.append(link[\"href\"])\n",
    "            \n",
    "        # Include all href that do not start with website link but with \"/\"\n",
    "        if str(link[\"href\"]).startswith(\"/\"):\n",
    "            if link[\"href\"] not in dict_href_links:\n",
    "                print(link[\"href\"])\n",
    "                dict_href_links[link[\"href\"]] = None\n",
    "                link_with_www = website_link + link[\"href\"][1:]\n",
    "                print(\"adjusted link =\", link_with_www)\n",
    "                list_links.append(link_with_www)\n",
    "\n",
    "        # Include all href that do not start with website link but without \"/\"\n",
    "        if str(link[\"href\"]).startswith(\"\"):\n",
    "           if link[\"href\"] not in dict_href_links:\n",
    "              print(link[\"href\"])\n",
    "              dict_href_links[link[\"href\"]] = None\n",
    "              link_with_www = website_link + link[\"href\"]\n",
    "              print(\"adjusted link =\", link_with_www)\n",
    "              list_links.append(link_with_www)\n",
    "          \n",
    "    # Convert list of links to dictionary and define keys as the links and the values as \"Not-checked\"\n",
    "    dict_links = dict.fromkeys(list_links, \"Not-checked\")\n",
    "    return dict_links\n",
    "\n",
    "def get_subpage_links(l):\n",
    "    for link in tqdm(l):\n",
    "        # If not crawled through this page start crawling and get links\n",
    "        if l[link] == \"Not-checked\":\n",
    "            dict_links_subpages = get_links(link) \n",
    "            # Change the dictionary value of the link to \"Checked\"\n",
    "            l[link] = \"Checked\"\n",
    "        else:\n",
    "            # Create an empty dictionary in case every link is checked\n",
    "            dict_links_subpages = {}\n",
    "        # Add new dictionary to old dictionary\n",
    "        l = {**dict_links_subpages, **l}\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add websuite WITH slash on end\n",
    "website = \"https://learn.microsoft.com/en-us/azure/machine-learning/\"\n",
    "# create dictionary of website\n",
    "dict_links = {website:\"Not-checked\"}\n",
    "\n",
    "counter, counter2 = None, 0\n",
    "while counter != 0:\n",
    "    counter2 += 1\n",
    "    dict_links2 = get_subpage_links(dict_links)\n",
    "    # Count number of non-values and set counter to 0 if there are no values within the dictionary equal to the string \"Not-checked\"\n",
    "    # https://stackoverflow.com/questions/48371856/count-the-number-of-occurrences-of-a-certain-value-in-a-dictionary-in-python\n",
    "    counter = sum(value == \"Not-checked\" for value in dict_links2.values())\n",
    "    # Print some statements\n",
    "    print(\"\")\n",
    "    print(\"THIS IS LOOP ITERATION NUMBER\", counter2)\n",
    "    print(\"LENGTH OF DICTIONARY WITH LINKS =\", len(dict_links2))\n",
    "    print(\"NUMBER OF 'Not-checked' LINKS = \", counter)\n",
    "    print(\"\")\n",
    "    dict_links = dict_links2\n",
    "    # Save list in json file\n",
    "    a_file = open(\"data.json\", \"w\")\n",
    "    json.dump(dict_links, a_file)\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all keys from dictionary\n",
    "list_links = list(dict_links.keys())\n",
    "list_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://learn.microsoft.com/en-us/azure/machine-learning/\"\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "for a_href in soup.find_all(\"a\", href=True):\n",
    "    print(a_href[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to scrape the content of a page\n",
    "def scrape_page(url):\n",
    "    # Get the page\n",
    "    page = requests.get(url)\n",
    "    # Create a soup object\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # extract all urls from the page that begin with the base url\n",
    "    urls = [url['href'] for url in soup.find_all('a', href=True) if url['href'].startswith(URL)]\n",
    "    return urls\n",
    "\n",
    "# Define a function to scrape the main text of a list of pages\n",
    "def scrape_pages(urls):\n",
    "    # Define a list to store the text\n",
    "    text = []\n",
    "    # Loop through the urls\n",
    "    for url in urls:\n",
    "        # Get the text for the url\n",
    "        page_text = scrape_page(url)\n",
    "        # Add the text to the list\n",
    "        text.append(page_text)\n",
    "    # Return the list\n",
    "    return text \n",
    "\n",
    "# Define a function to scrape the main text of a list of pages and save it to a file\n",
    "def scrape_pages_to_json(urls, file_name):\n",
    "    # Get the text\n",
    "    text = scrape_pages(urls)\n",
    "    # Save the text to a file\n",
    "    df = pd.DataFrame(text)\n",
    "    df.to_json(file_name)\n",
    "\n",
    "scrape_pages_to_json(list_links, 'data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb4a0ac80907d7f44e1a5e88d3d3381b33e3dbedd3a24d113e876f30a0c46bee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
